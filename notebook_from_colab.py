# -*- coding: utf-8 -*-
"""Tabular_Data_Science_Final_Project_Tomer_and_Ziv.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xjEidVWYJ2dR37RJGmoBowhIK8lOktXv

"""


"""#Imports"""

import shap
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.utils import shuffle
from sklearn.preprocessing import LabelEncoder, OneHotEncoder
from sklearn import neighbors
from sklearn.metrics import classification_report
from sklearn import svm as skl_svm
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn import preprocessing
from sklearn.model_selection import StratifiedShuffleSplit
from xgboost import XGBRegressor,XGBClassifier
from sklearn.inspection import permutation_importance
from matplotlib import pyplot
from xgboost import plot_importance

"""#Load the datasets

Dataset 1 - [Iris Species](https://www.kaggle.com/datasets/uciml/iris): Data of different iris plants classified into three species.

Dataset 2 - [Titanic passengers](https://www.kaggle.com/competitions/titanic/data) : This data contains the detailes of the passengers classified to survived or to not survived from the disaster.

Dataset 3 - [Cancer detection](https://www.kaggle.com/datasets/uciml/breast-cancer-wisconsin-data): Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.

Dataset 4 - [Stress detection in sleep](https://www.kaggle.com/datasets/laavanya/human-stress-detection-in-and-through-sleep): Sleeping parameters captured for sleep study be an IoT sensor.
"""

IRIS_URL = "https://raw.githubusercontent.com/ZivBenda/tabular_data_science_biu_final_project/main/datasets/IRIS.csv"
TITANIC_URL = "https://raw.githubusercontent.com/ZivBenda/tabular_data_science_biu_final_project/main/datasets/titanic.csv"
CANCER_URL = "https://raw.githubusercontent.com/ZivBenda/tabular_data_science_biu_final_project/main/datasets/cancer.csv"
SLEEPING_URL = "https://raw.githubusercontent.com/ZivBenda/tabular_data_science_biu_final_project/main/datasets/sleeping.csv"

IRIS_DATASET = "iris_dataset"
TITANIC_DATASET = "titanic_dataset"
CANCER_DATASET = "cancer_dataset"
SLEEPING_DATASET = "sleeping_dataset"

SEED = 2023
np.random.seed(SEED)

def reload_datasets():
    return { IRIS_DATASET: pd.read_csv(IRIS_URL),
             TITANIC_DATASET: pd.read_csv(TITANIC_URL),
             SLEEPING_DATASET: pd.read_csv(SLEEPING_URL),
             CANCER_DATASET: pd.read_csv(CANCER_URL)
           }

dtfs = reload_datasets()

"""# Models Definitions
here we creating 4 model trainig methods (KNN, SVM, Decision_Tree & Logistic Regression with shap plots.
"""

def KNN(X_train, Y_train, X_test, dataset_name, class_names):
    print(f'Executing KNN on {dataset_name}')
    plt.subplot(121)
    n_neighbors = 15
    knn = neighbors.KNeighborsClassifier(n_neighbors, weights='distance')
    knn.fit(X_train.values, Y_train)
    knn_explainer = shap.KernelExplainer(knn.predict, X_test, seed=SEED)
    knn_shap_values = knn_explainer.shap_values(X_test)
    shap.summary_plot(knn_shap_values, X_test, show=False)
    plt.savefig(f'{dataset_name}_KNN.png')

    y_predict = knn.predict(X_test)
    return y_predict

def SVM(X_train, Y_train, X_test, dataset_name, class_names):
    print(f'Executing SVM on {dataset_name}')
    plt.subplot(121)
    svm = skl_svm.SVC(gamma='scale', decision_function_shape='ovo')
    svm.fit(X_train.values, Y_train)
    svm_explainer = shap.KernelExplainer(svm.predict, X_test, seed=SEED)
    svm_shap_values = svm_explainer.shap_values(X_test)
    shap.summary_plot(svm_shap_values, X_test, show=False)
    plt.savefig(f'{dataset_name}_SVM.png')

    y_predict = svm.predict(X_test)
    return y_predict

def Decision_Tree(X_train, Y_train, X_test, dataset_name, class_names):
    print(f'Executing Decision_Tree on {dataset_name}')
    model = DecisionTreeClassifier(random_state=1, max_depth=5)
    model.fit(X_train, Y_train)
    decision_tree_explainer = shap.TreeExplainer(model, seed=SEED)
    decision_tree_shap_values = decision_tree_explainer.shap_values(X_test)
    shap.summary_plot(decision_tree_shap_values, X_test, show=False, class_names=class_names)
    plt.savefig(f'{dataset_name}_decision_tree.png')
    # shap.force_plot(explainer.expected_value[0], shap_values[0], X_test.iloc[0])
    '''
    linear_reg_explainer = shap.KernelExplainer(linear_model.predict, X_test)
    linear_reg_shap_values = linear_reg_explainer.shap_values(X_test)
    shap.summary_plot(linear_reg_shap_values, X_test, show=False)
    plt.savefig(f'{dataset_name}_linear_regression.png')
    '''
    y_predict = model.predict(X_test)
    return y_predict

def Logistic_Regression(X_train, Y_train, X_test, dataset_name, class_names):
    print(f'Executing Logistic_Regression on {dataset_name}')
    logistic_reg_model = LogisticRegression(solver='lbfgs')
    logistic_reg_model.fit(X_train, Y_train)
    logistic_reg_explainer = shap.KernelExplainer(logistic_reg_model.predict, X_test, seed=SEED)
    logistic_reg_shap_values = logistic_reg_explainer.shap_values(X_test)
    shap.summary_plot(logistic_reg_shap_values, X_test, show=False)
    plt.savefig(f'{dataset_name}_logistic_regression.png')

    y_predict = logistic_reg_model.predict(X_test)
    return y_predict

"""# models dictionary
a wrapper for our classification models methods mentioned above
"""

models={'KNN' : lambda X_train, Y_train, X_test, dataset_name, class_names : KNN(X_train, Y_train, X_test, dataset_name, class_names),
        'SVM' : lambda X_train, Y_train, X_test, dataset_name, class_names : SVM(X_train, Y_train, X_test, dataset_name, class_names),
        'Decision_Tree' : lambda X_train, Y_train, X_test, dataset_name, class_names : Decision_Tree(X_train, Y_train, X_test, dataset_name, class_names),
        'Logistic_Regression' : lambda X_train, Y_train, X_test, dataset_name, class_names:Logistic_Regression(X_train.values, Y_train, X_test, dataset_name, class_names)
        }

"""#XGBOOST model

We try to compare between the results of SHAP with the above models and the result of the tree-based XGBOOST model.
"""

def XGBoostClassifier(X_train, Y_train, dataset_name):
    xgb = XGBClassifier()
    xgb.fit(X_train, Y_train)
    # plot feature importance
    plot_importance(xgb)
    pyplot.show()
    plt.savefig(f'{dataset_name}_XGBOOST.png')

"""#Preprocess the datasets
selecting only potential columns for classification task i.e ignoring (id, name) coulums etc.
"""

def preprocess_dataset(name, dataset):
    features = []
    if name == "iris_dataset":
        Y = dataset['species']
        features = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']
        X = dataset[features]

    elif name == "titanic_dataset":
        Y = dataset['Survived']
        # Ommiting Passenger-id, Name and Ticket columns
        features = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Cabin', 'Embarked']
        X = dataset[features]

    elif name == "sleeping_dataset":
        dataset['sl'] = dataset['sl'].astype(int)
        Y = dataset['sl']
        features = ['sr1', 'rr', 't', 'lm', 'bo', 'rem', 'sr2', 'hr']
        X = dataset[features]

    elif name == "cancer_dataset":
        Y = dataset['diagnosis']
        # Ommiting id and name
        features = ['radius_mean', 'texture_mean', 'perimeter_mean', 'area_mean',
                     'smoothness_mean', 'compactness_mean', 'concavity_mean',
                     'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',
                     'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',
                     'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',
                     'fractal_dimension_se', 'radius_worst', 'texture_worst',
                     'perimeter_worst', 'area_worst', 'smoothness_worst',
                     'compactness_worst', 'concavity_worst', 'concave points_worst',
                     'symmetry_worst', 'fractal_dimension_worst']
        X = dataset[features]
    return(X, Y)

"""#Models training method
this method purpose is to train all models on specific dataset and print classification report for each of them.
"""

shap.initjs()

def models_train(X_train, y_train, X_test, y_test, dataset_name, target_strings):
    for model in models.values():
        y_predict = model(X_train, y_train, X_test, dataset_name, target_strings)

        print(classification_report(y_test, y_predict, target_names=target_strings))
        plt.show()

"""# EDA per datasets and models training
in this section we will run some general statistics per dataset i.e (check for missing values, encode string labels into numbers, split the data while perserving the same ratio labels between train & test sets).

## Iris Dataset

### EDA IRIS - Data Exploration
"""

dataset = dtfs[IRIS_DATASET]
dataset.head()

"""### labels organization"""

print(*list(dataset['species']))

"""We can see that the column 'species' is ordered, so we shuffle it"""

dataset = shuffle(dataset)

print(*list(dataset['species']))

(X,Y) = preprocess_dataset(IRIS_DATASET, dataset)

print(X.head(5))

"""### checking for missing values"""

print(X.isnull().sum())

Y.value_counts()

plt.pie(Y.value_counts(),labels=Y.unique(),autopct = '%1.2f%%')

"""### StratifiedShuffleSplit

We splits the data using StratifiedShuffleSplit in order to perseve the same labels ratio between train & test.
"""

splitter=StratifiedShuffleSplit(n_splits=1,random_state=12, test_size=0.2) #we can make a number of combinations of split
#But we are interested in only one.

for train,test in splitter.split(X, Y):     #this will splits the index
    X_train = X.iloc[train]
    y_train = Y.iloc[train]
    X_test = X.iloc[test]
    y_test = Y.iloc[test]
print(y_train.value_counts())
print(y_test.value_counts())

plt.figure(figsize=(10,15))

plt.subplot(121)
plt.pie(y_train.value_counts(),labels=y_train.unique(),autopct = '%1.2f%%')
plt.title('Training Dataset')

plt.subplot(122)
plt.pie(y_test.value_counts(),labels=y_test.unique(),autopct = '%1.2f%%')
plt.title('Test Dataset')

plt.tight_layout()

"""### Transforming the y string label using LabelEnconder
we also need to make sure we set the same label encoding for both y_train & y_test
"""

le = preprocessing.LabelEncoder()
trained_le = le.fit(y_train)

y_train = trained_le.transform(y_train)
y_test = trained_le.transform(y_test)

"""### Get Class Names"""

Y.unique()

print(len(Y.unique()))
target_strings = le.inverse_transform(np.arange(len(Y.unique())))
target_strings

"""###Train models with SHAP"""

models_train(X_train, y_train, X_test, y_test, IRIS_DATASET, target_strings)

"""###XGBOOST"""

XGBoostClassifier(X_train, y_train, IRIS_DATASET)

"""###Feature Selection
From the graphs above we can see that half of the models ranked 'sepal length' as the least important, and half of the models ranked 'sepal width' as the least important. However, the XGBOOST model ranked 'sepal length' as the least important too. Therefore, we choose to select all features except 'sepal length' and train the models again.

"""

X_train = X_train.drop(columns=['sepal_length'])
X_test = X_test.drop(columns=['sepal_length'])

models_train(X_train, y_train, X_test, y_test, IRIS_DATASET, target_strings)

"""## Titanic dataset

### EDA TITANIC - Data Exploration
"""

dataset = dtfs[TITANIC_DATASET]
dataset.head()

(X,Y) = preprocess_dataset(TITANIC_DATASET, dataset)

"""### checking for missing values"""

print(X.isnull().sum())

"""the Cabin column has too many NA values, hence its better to be dropped

the NA values of Age can be filled by mean imputation, and of Embarked by mode imputation
"""

X['Age'].fillna(X['Age'].mean(), inplace=True)
X['Embarked'].fillna(X['Embarked'].mode()[0], inplace=True)
X.drop(['Cabin'], axis=1, inplace=True)

"""### Features and Labels Organization

We should convert the categorical data values into numeric.
"""

X['Sex'] = LabelEncoder().fit_transform(X['Sex'])
X['Embarked'] = LabelEncoder().fit_transform(X['Embarked'])

print(*list(dataset['Survived']))

"""We can see that the column 'Survived' is already encoded"""

print(X.head(5))

Y.value_counts()

plt.pie(Y.value_counts(),labels=Y.unique(),autopct = '%1.2f%%')

"""### StratifiedShuffleSplit

We splits the data using StratifiedShuffleSplit in order to perseve the same labels ratio between train & test.
"""

splitter=StratifiedShuffleSplit(n_splits=1,random_state=12, test_size=0.2) #we can make a number of combinations of split
#But we are interested in only one.

for train,test in splitter.split(X, Y):     #this will splits the index
    X_train = X.iloc[train]
    y_train = Y.iloc[train]
    X_test = X.iloc[test]
    y_test = Y.iloc[test]
print(y_train.value_counts())
print(y_test.value_counts())

plt.figure(figsize=(10,15))

plt.subplot(121)
plt.pie(y_train.value_counts(),labels=y_train.unique(),autopct = '%1.2f%%')
plt.title('Training Dataset')

plt.subplot(122)
plt.pie(y_test.value_counts(),labels=y_test.unique(),autopct = '%1.2f%%')
plt.title('Test Dataset')

plt.tight_layout()

"""### Transforming the y string label using LabelEnconder
we also need to make sure we set the same label encoding for both y_train & y_test
"""

le = preprocessing.LabelEncoder()
trained_le = le.fit(y_train)

y_train = trained_le.transform(y_train)
y_test = trained_le.transform(y_test)

"""### Get Class Names"""

Y.unique()

target_strings = np.array(['Not Survived', 'Survived'])
target_strings

"""###Train models with SHAP"""

models_train(X_train, y_train, X_test, y_test, TITANIC_DATASET, target_strings)

"""###XGBOOST"""

XGBoostClassifier(X_train, y_train, TITANIC_DATASET)

"""###Feature Selection
From the graphs above its seems that the feature 'parch' is less important. Therefore, we choosh to select all features without 'parch' and train the models again.

"""

X_train = X_train.drop(columns=['Parch'])
X_test = X_test.drop(columns=['Parch'])

models_train(X_train, y_train, X_test, y_test, TITANIC_DATASET, target_strings)

"""## Sleeping dataset

### EDA SLEEPING - Data Exploration
"""

dataset = dtfs[SLEEPING_DATASET]
dataset.head()

"""### labels organization"""

print(*list(dataset['sl']))

(X,Y) = preprocess_dataset(SLEEPING_DATASET, dataset)

print(X.head(5))

"""### checking for missing values"""

print(X.isnull().sum())

Y.value_counts()

plt.pie(Y.value_counts(),labels=Y.unique(),autopct = '%1.2f%%')

"""### StratifiedShuffleSplit

We splits the data using StratifiedShuffleSplit in order to perseve the same labels ratio between train & test.
"""

splitter=StratifiedShuffleSplit(n_splits=1,random_state=12, test_size=0.2) #we can make a number of combinations of split
#But we are interested in only one.

for train,test in splitter.split(X, Y):     #this will splits the index
    X_train = X.iloc[train]
    y_train = Y.iloc[train]
    X_test = X.iloc[test]
    y_test = Y.iloc[test]
print(y_train.value_counts())
print(y_test.value_counts())

plt.figure(figsize=(10,15))

plt.subplot(121)
plt.pie(y_train.value_counts(),labels=y_train.unique(),autopct = '%1.2f%%')
plt.title('Training Dataset')

plt.subplot(122)
plt.pie(y_test.value_counts(),labels=y_test.unique(),autopct = '%1.2f%%')
plt.title('Test Dataset')

plt.tight_layout()

"""### Transforming the y string label using LabelEnconder
we also need to make sure we set the same label encoding for both y_train & y_test
"""

le = preprocessing.LabelEncoder()
trained_le = le.fit(y_train)

y_train = trained_le.transform(y_train)
y_test = trained_le.transform(y_test)

"""###Train models with SHAP"""

Y.unique()

target_strings = np.array(['3', '1', '0', '2', '4'])  # Y.unique()
models_train(X_train, y_train, X_test, y_test, SLEEPING_DATASET, target_strings)

"""###XGBOOST"""

XGBoostClassifier(X_train, y_train, SLEEPING_DATASET)

"""###Feature Selection
From the graphs above we can see that the models ranked 'rr' as the least important. Therefore, we choosh to select all features except 'rr' and train the models again.

"""

X_train = X_train.drop(columns=['rr'])
X_test = X_test.drop(columns=['rr'])

models_train(X_train, y_train, X_test, y_test, SLEEPING_DATASET, target_strings)

"""## Cancer dataset

### EDA CANCER - Data Exploration
"""

dataset = dtfs[CANCER_DATASET]
dataset.head()

"""### labels organization"""

print(*list(dataset['diagnosis']))

"""shuffle diagnosis column"""

dataset = shuffle(dataset)

print(*list(dataset['diagnosis']))

(X,Y) = preprocess_dataset(CANCER_DATASET, dataset)

print(X.head(5))

"""### checking for missing values"""

print(X.isnull().sum())

Y.value_counts()

plt.pie(Y.value_counts(),labels=Y.unique(),autopct = '%1.2f%%')

"""### StratifiedShuffleSplit

We splits the data using StratifiedShuffleSplit in order to perseve the same labels ratio between train & test.
"""

splitter=StratifiedShuffleSplit(n_splits=1,random_state=12, test_size=0.2) #we can make a number of combinations of split
#But we are interested in only one.

for train,test in splitter.split(X, Y):     #this will splits the index
    X_train = X.iloc[train]
    y_train = Y.iloc[train]
    X_test = X.iloc[test]
    y_test = Y.iloc[test]
print(y_train.value_counts())
print(y_test.value_counts())

plt.figure(figsize=(10,15))

plt.subplot(121)
plt.pie(y_train.value_counts(),labels=y_train.unique(),autopct = '%1.2f%%')
plt.title('Training Dataset')

plt.subplot(122)
plt.pie(y_test.value_counts(),labels=y_test.unique(),autopct = '%1.2f%%')
plt.title('Test Dataset')

plt.tight_layout()

"""### Transforming the y string label using LabelEnconder
we also need to make sure we set the same label encoding for both y_train & y_test
"""

le = preprocessing.LabelEncoder()
trained_le = le.fit(y_train)

y_train = trained_le.transform(y_train)
y_test = trained_le.transform(y_test)

"""###Train models with SHAP"""

target_strings = Y.unique()
target_strings

target_strings = Y.unique()
models_train(X_train, y_train, X_test, y_test, CANCER_DATASET, target_strings)

"""###XGBOOST"""

XGBoostClassifier(X_train, y_train, CANCER_DATASET)

"""###Feature Selection
From the graphs above we can see that there are some features that even not appeared in the top 20 important features. Therefore, we choosh to select all features without the following:


*   compactness_worst
*   concave points_se
*   concavity_se
*   fractal_dimension_se
*   radius_mean
*   smoothness_worst
*   symmetry_worst


"""

X_train = X_train.drop(columns=['compactness_worst', 'concave points_se', 'concavity_se', 'fractal_dimension_se', 'radius_mean', 'smoothness_worst', 'symmetry_worst'])
X_test = X_test.drop(columns=['compactness_worst', 'concave points_se', 'concavity_se', 'fractal_dimension_se', 'radius_mean', 'smoothness_worst', 'symmetry_worst'])

models_train(X_train, y_train, X_test, y_test, CANCER_DATASET, target_strings)